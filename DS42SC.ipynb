{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS42SC.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wel51x/DS-Unit-4-Sprint-2-NLP/blob/master/DS42SC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QpPcbYew_ttN"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Sprint Challenge*\n",
        "\n",
        "# Natural Language Processing\n",
        "\n",
        "**Part 1 - Working with Text Data**\n",
        "Use Python string methods remove irregular whitespace from the following string:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHEDWtF4OnE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from gensim.models.word2vec import Word2Vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diJKJQZnOpBi",
        "colab_type": "code",
        "outputId": "b004bfa0-644d-448e-eef7-d34a7d6c205c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Downloads\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dtotEnsStY5o",
        "outputId": "7991a772-11c7-412d-9d89-adcf38c7268b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "whitespace_string = \"\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   \"\n",
        "\n",
        "print(whitespace_string)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  This is a    string   that has  \n",
            " a lot of  extra \n",
            "   whitespace.   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G9-MkBwasXx8",
        "outputId": "3f4b8f82-6f68-4fdf-ceb6-161dab640635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "my_str = re.sub('[\\s]+', ' ', whitespace_string.strip())\n",
        "my_str.replace('has a lot of', 'doesn\\'t have')"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"This is a string that doesn't have extra whitespace.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vg1-d2aAsXLn"
      },
      "source": [
        "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
        "\n",
        "[RegEx dates.txt](https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt)\n",
        "\n",
        "- Day\n",
        "- Month\n",
        "- Year\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KWDiN4C9_0sq",
        "outputId": "e753c63f-06a1-445b-98d5-d6e051fc60ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        }
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt',\n",
        "                names = [\"Month\", \"Day\", \"Year\"], header = None, sep = \"(?:,|\\s)\\s*\")\n",
        "df"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Month</th>\n",
              "      <th>Day</th>\n",
              "      <th>Year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>March</td>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>March</td>\n",
              "      <td>15</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>March</td>\n",
              "      <td>22</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>March</td>\n",
              "      <td>29</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>April</td>\n",
              "      <td>5</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>April</td>\n",
              "      <td>12</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>April</td>\n",
              "      <td>19</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>April</td>\n",
              "      <td>26</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>May</td>\n",
              "      <td>3</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>May</td>\n",
              "      <td>10</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>May</td>\n",
              "      <td>17</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>May</td>\n",
              "      <td>24</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>May</td>\n",
              "      <td>31</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>June</td>\n",
              "      <td>7</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>June</td>\n",
              "      <td>14</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>June</td>\n",
              "      <td>21</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>June</td>\n",
              "      <td>28</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>July</td>\n",
              "      <td>5</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>July</td>\n",
              "      <td>12</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>July</td>\n",
              "      <td>19</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Month  Day  Year\n",
              "0   March    8  2015\n",
              "1   March   15  2015\n",
              "2   March   22  2015\n",
              "3   March   29  2015\n",
              "4   April    5  2015\n",
              "5   April   12  2015\n",
              "6   April   19  2015\n",
              "7   April   26  2015\n",
              "8     May    3  2015\n",
              "9     May   10  2015\n",
              "10    May   17  2015\n",
              "11    May   24  2015\n",
              "12    May   31  2015\n",
              "13   June    7  2015\n",
              "14   June   14  2015\n",
              "15   June   21  2015\n",
              "16   June   28  2015\n",
              "17   July    5  2015\n",
              "18   July   12  2015\n",
              "19   July   19  2015"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s4Q0dgoe_uBW"
      },
      "source": [
        "# Part 2 - Bag of Words \n",
        "\n",
        "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
        "\n",
        "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
        "\n",
        " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
        "\n",
        "1) Text should be lowercase.\n",
        "\n",
        "2) Stopwords should be removed.\n",
        "\n",
        "3) Punctuation should be removed.\n",
        "\n",
        "4) Tweets should be tokenized at the word level. \n",
        "\n",
        "(The above don't necessarily need to be completed in that specific order.)\n",
        "\n",
        "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1xzdhyTS_3F9",
        "outputId": "2b850bb2-fff4-40b3-cb4b-61eb3ebbb023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "twitter = pd.read_csv(\"https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv\")\n",
        "print(twitter.shape)\n",
        "twitter.head(11)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(99989, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>is so sad for my APL frie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>I missed the New Moon trail...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>omg its already 7:30 :O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>or i just worry too much?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>Juuuuuuuuuuuuuuuuussssst Chillin!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>Sunny Again        Work Tomorrow  :-|  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>handed in my uniform today . i miss you ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>hmmmm.... i wonder how she my number @-)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>I must think about positive..</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Sentiment                                      SentimentText\n",
              "0           0                       is so sad for my APL frie...\n",
              "1           0                     I missed the New Moon trail...\n",
              "2           1                            omg its already 7:30 :O\n",
              "3           0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
              "4           0           i think mi bf is cheating on me!!!   ...\n",
              "5           0                  or i just worry too much?        \n",
              "6           1                 Juuuuuuuuuuuuuuuuussssst Chillin!!\n",
              "7           0         Sunny Again        Work Tomorrow  :-|  ...\n",
              "8           1        handed in my uniform today . i miss you ...\n",
              "9           1           hmmmm.... i wonder how she my number @-)\n",
              "10          0                      I must think about positive.."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmbZ_esIRFpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stop words\n",
        "stop = stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wvwv7oTPPVAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_twitter(in_text):\n",
        "  return ''.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\ / \\ / \\S+)\",\"\", in_text.lower()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y4Bq9iQPUi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean text\n",
        "twitter['SentimentText'] = twitter['SentimentText'].apply(clean_twitter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7R9P7VkPUPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twitter = twitter.rename(columns={\"SentimentText\": \"CleanedText\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNjNVen-RczY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove stop words\n",
        "twitter['CleanedText'] = twitter['CleanedText'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPr0bd-RQW_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize\n",
        "twitter['Tokens'] = twitter['CleanedText'].apply(word_tokenize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDUiNwctQlJg",
        "colab_type": "code",
        "outputId": "21154198-4113-4323-cd89-a23d85a20e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "twitter.head(11)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>CleanedText</th>\n",
              "      <th>Tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>sad apl friend</td>\n",
              "      <td>[sad, apl, friend]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>missed new moon trailer</td>\n",
              "      <td>[missed, new, moon, trailer]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>omg already 730</td>\n",
              "      <td>[omg, already, 730]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>omgaga im sooo im gunna cry ive dentist since ...</td>\n",
              "      <td>[omgaga, im, sooo, im, gunna, cry, ive, dentis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>think mi bf cheating tt</td>\n",
              "      <td>[think, mi, bf, cheating, tt]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>worry much</td>\n",
              "      <td>[worry, much]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>juuuuuuuuuuuuuuuuussssst chillin</td>\n",
              "      <td>[juuuuuuuuuuuuuuuuussssst, chillin]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>sunny work tomorrow tv tonight</td>\n",
              "      <td>[sunny, work, tomorrow, tv, tonight]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>handed uniform today miss already</td>\n",
              "      <td>[handed, uniform, today, miss, already]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>hmmmm wonder number</td>\n",
              "      <td>[hmmmm, wonder, number]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>must think positive</td>\n",
              "      <td>[must, think, positive]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Sentiment                                        CleanedText  \\\n",
              "0           0                                     sad apl friend   \n",
              "1           0                            missed new moon trailer   \n",
              "2           1                                    omg already 730   \n",
              "3           0  omgaga im sooo im gunna cry ive dentist since ...   \n",
              "4           0                            think mi bf cheating tt   \n",
              "5           0                                         worry much   \n",
              "6           1                   juuuuuuuuuuuuuuuuussssst chillin   \n",
              "7           0                     sunny work tomorrow tv tonight   \n",
              "8           1                  handed uniform today miss already   \n",
              "9           1                                hmmmm wonder number   \n",
              "10          0                                must think positive   \n",
              "\n",
              "                                               Tokens  \n",
              "0                                  [sad, apl, friend]  \n",
              "1                        [missed, new, moon, trailer]  \n",
              "2                                 [omg, already, 730]  \n",
              "3   [omgaga, im, sooo, im, gunna, cry, ive, dentis...  \n",
              "4                       [think, mi, bf, cheating, tt]  \n",
              "5                                       [worry, much]  \n",
              "6                 [juuuuuuuuuuuuuuuuussssst, chillin]  \n",
              "7                [sunny, work, tomorrow, tv, tonight]  \n",
              "8             [handed, uniform, today, miss, already]  \n",
              "9                             [hmmmm, wonder, number]  \n",
              "10                            [must, think, positive]  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q764vszGqiUh"
      },
      "source": [
        "### How should TF-IDF scores be interpreted? How are they calculated?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e2Ji7BMhqs3M"
      },
      "source": [
        "TF-IDF (Term Frequency - Inverse Document Frequency) scores help in determining the importance of a word in a document. The idea is that the more often a word occurs, the more important it is.\n",
        "\n",
        "However, sometimes a word so frequently in a corpus of documents that its importance is diminished to the point where it becomes trivial, and should be removed via stop words.\n",
        "\n",
        "For example, if the word \"criminal\" occurs several times in a document, it's likely to be important. However if I'm looking through the district attorney's files, that word is going to be less important as it will likely appear in most - if not all - documents.\n",
        "\n",
        "Term Frequency (the TF in TF-IDF) is the number of times a word occurs in a given document.\n",
        "\n",
        "Inverse Document Frequency (the IDF in TF-IDF) is calculated as the log of the total number of documents containing the expression divided by the number of documents in the corpus. containing the expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3iUeBKtG_uEK"
      },
      "source": [
        "# Part 3 - Document Classification\n",
        "\n",
        "1) Use Train_Test_Split to create train and test datasets.\n",
        "\n",
        "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
        "\n",
        " - Stretch goal: Use both of the methods that we talked about in class.\n",
        "\n",
        "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
        "\n",
        "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
        "\n",
        " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
        " - Stretch goal: Track your results in a DataFrmae and produce a visualization of the results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TX8OEgUP_3ee",
        "outputId": "13d6d080-7d67-43cb-838a-885fa6fdce1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Split\n",
        "X = twitter['CleanedText']\n",
        "y = twitter['Sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(79991,)\n",
            "(19998,)\n",
            "(79991,)\n",
            "(19998,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVRIe6kVUxeU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CountVectorizer\n",
        "count_vectorizer = CountVectorizer(stop_words = stop)\n",
        "# Learn vocabulary from training texts and vectorize training & test texts.\n",
        "X_train_word_counts = count_vectorizer.fit_transform(X_train)\n",
        "X_test_word_counts = count_vectorizer.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxY3gDh-VWJY",
        "colab_type": "code",
        "outputId": "10e276b9-2028-478f-a577-76ed0e23e592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "# LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train_word_counts, y_train)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvJvM4kVVn8G",
        "colab_type": "code",
        "outputId": "cb2afb86-d3fa-48b2-87f8-bb7831a761fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Train ROC AUC:\", roc_auc_score(lr.predict(X_train_word_counts), y_train))\n",
        "print(\"Test ROC AUC:\", roc_auc_score(lr.predict(X_test_word_counts), y_test))"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train ROC AUC: 0.863090008294845\n",
            "Test ROC AUC: 0.7535332144732607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coxtRxATYY0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MultinomialNB\n",
        "mnb = MultinomialNB().fit(X_train_counts, y_train)\n",
        "mnb.fit(X_train_counts, y_train);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIsxIZwRYmcT",
        "colab_type": "code",
        "outputId": "918d6aa0-e477-41f7-a64e-546f27c744e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Train ROC AUC:\", roc_auc_score(mnb.predict(X_train_counts), y_train))\n",
        "print(\"Test ROC AUC:\", roc_auc_score(mnb.predict(X_test_counts), y_test))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train ROC AUC: 0.7956634747534105\n",
            "Test ROC AUC: 0.49733040753660973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRuK01KeZboa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words = stop)\n",
        "# Learn vocabulary from training texts and vectorize training & test texts.\n",
        "X_train_tfidf_word_counts = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf_word_counts = tfidf_vectorizer.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTr3zL0BZ4ux",
        "colab_type": "code",
        "outputId": "4bbfff2d-7ee0-4d50-a452-7af94b47ed06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "# LogisticRegression\n",
        "LR = LogisticRegression()\n",
        "LR.fit(X_train_tfidf_word_counts, y_train)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_L548zvaDy_",
        "colab_type": "code",
        "outputId": "a33f06fb-c6cb-452d-81cd-625b7340fb55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Train ROC AUC:\", roc_auc_score(LR.predict(X_train_tfidf_word_counts), y_train))\n",
        "print(\"Test ROC AUC:\", roc_auc_score(LR.predict(X_test_tfidf_word_counts), y_test))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train ROC AUC: 0.8223077314860625\n",
            "Test ROC AUC: 0.7580907648198792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ6u70fbaVhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MultinomialNB\n",
        "MNB = MultinomialNB().fit(X_train_counts, y_train)\n",
        "MNB.fit(X_train_tfidf_word_counts, y_train);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKbUI3u2apj5",
        "colab_type": "code",
        "outputId": "bfeda51b-dd05-41e0-9369-e284cff46eea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Train ROC AUC:\", roc_auc_score(MNB.predict(X_train_tfidf_word_counts), y_train))\n",
        "print(\"Test ROC AUC:\", roc_auc_score(MNB.predict(X_test_tfidf_word_counts), y_test))"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train ROC AUC: 0.8499163604979155\n",
            "Test ROC AUC: 0.7479045902750181\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sorF95UO_uGx"
      },
      "source": [
        "# Part 4 -  Word2Vec\n",
        "\n",
        "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
        "\n",
        "2) Display the 10 words that are most similar to the word \"twitter\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DYno4d4N-LHR",
        "outputId": "3852e5c9-b491-4541-fa2f-2c17274d16a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "model = Word2Vec(twitter['Tokens'])\n",
        "model.most_similar('twitter')"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('facebook', 0.8121285438537598),\n",
              " ('following', 0.7890974879264832),\n",
              " ('dm', 0.7877471446990967),\n",
              " ('list', 0.7842813730239868),\n",
              " ('updates', 0.775046169757843),\n",
              " ('page', 0.7749948501586914),\n",
              " ('blog', 0.7691603899002075),\n",
              " ('site', 0.7680405974388123),\n",
              " ('email', 0.7661299705505371),\n",
              " ('message', 0.7659568190574646)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    }
  ]
}